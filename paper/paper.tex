\documentclass[12pt,a4paper]{article}
% vim: set textwidth=100:

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{hyperref}

\title{Data Structures Comparative Review Thing}
\author{Pieter Brederode, Timo Koppenberg, \\ Jelco Bodewes, Anton Golov}

\begin{document}
    \maketitle

    \begin{abstract}
        We did research and got results~\cite{CallOfCthulhu}.
    \end{abstract}


    \section{Introduction}

    We did research about what data structure could best be used to represent a blacklist in a 
    web-filter application.  We compared the insertion speed, query speed, and memory usage of three
    data structures: the binary search tree, the hash table, and the skip list.  We did this because
    we really liked these three structures. 

    \section{Implementation}

    We used C++ as the implementation language.  libstdc++'s implementations of the standard class
    templates \texttt{std::map} and \texttt{std::unordered\_map} were used for the binary search
    tree and the hash table respectively.  As the C++ standard library lacks a skip
    list\footnote{reference to the standard here}, we used an open-source third party library,
    \texttt{CSSkipList}\footnote{reference to library home page here}.  We also used libstdc++'s
    implementation of \texttt{std::string} for our string handling needs.

    For our measurements, we used libstdc++'s implementation of the standard
    \texttt{std::chrono::high\_resolution\_clock} class was used for time measurements.   Memory
    measurements were performed by providing a custom allocator which tracked the number of memory
    requests.  Note that the latter is a step away from our initial plan, where we stated we would
    use the POSIX API\footnote{ref to the posix api}; unfortunately, that turned out to be too
    coarse-grained for our purposes and we could thus get no accurate measurements that way.  The
    allocator-based technique we employed required slight modifications to the skip list
    implementation we used, but this shouldn't have caused a significant slowdown (if anything, it
    made things more fair, as now allocation was done the same way in both systems).

    For our data, we took the system's dict file and the list of top level domains from Wikipedia
    and combined the two to generate a bunch of things of the form \texttt{www.\$word.\$tld}, where
    \texttt{\$word} is one of the dictionary words and \texttt{\$tld} is a top level domain.  Only
    words consisting of the lowercase letters \texttt{a} through \texttt{z} were used.  Words were
    limited to be from 8 to 12 characters and top-level domains were limited to exactly two letters.
    All possible combinations were generated.  For the full paths, a number of domains were selected
    and each was appended some number of random alphanumeric strings of eighty characters.  For more
    details, see the code.

    After some test runs we chose to decrease the number of runs used for measurements.  In the
    proposal we had said that for each time measurement we would perform the relevant operation 1000
    times and then take the average of that.  In practice, this made the tests run for longer than
    we could afford and the results given by running 100 times were comparable so we only ran that
    many times.  We expect the general result to be the same; see some later section for details
    about what \emph{could} have gone wrong.

    \section{Results}

    Our results were great!  They are best analysed with a two-sample independent T test.  We had an
    equal number of measurements about group 1 (with mean $x_1$ and standard deviation $\sigma_1$ and
    about group 2 (with mean $x_2$ and standard deviation $\sigma_2$).  From this we can conclude that
    data structure 1 is, on average, unquestionably superior.

    \subsection{Possible errors caused by the implementation being not entirely perfect}

    We really did our best to make sure these kinds of things didn't happen.  However, there were
    still a number of measurements that do not follow from the underlying theory and are likely a
    consequence of the hardware involved.

    In particular, constructing a small hash table after constructing many big ones takes an
    absurdly long time.  Only the first hash table is affected, and the time is comparable to the
    time required to construct a 4k element hash table, which is just silly.  It probably has
    something to do with the allocator but the reason for such behaviour is still unclear.  The fact
    a hashtable needs contiguous memory probably has something to do with it, as well as the
    allocator returning memory to the operating system.  Still, the latter should also have happened
    with the other structures.  Bizarre.

    A similar issue occurs around 4k or 8k (depending on data structure).  In all three cases, the
    cost of insertion and querying grows at a higher rate than previously.  This is difficult to
    quantify\footnote{Can we do some sort of linear regression to prove this is the case?}, but it
    seems like some extra costs are added around then.  We suspect it is related either to the
    allocator having to get more memory or the CPU cache running out and more cache misses occuring.
    We'd measure that too if we had more time/resources.

    A possibly related issue is that the standard deviation of our measurements rises around that
    point.  That strengthens the possibility of it being allocator-related, as that would involve a
    system call and could cause scheduling issues (despite the system being otherwise very close to
    idle).

    Also, we have counterintuitive results that show that inserting or querying a full path is
    faster than querying only the domain name.  This can partially be explained\footnote{Not really,
    but ehhh.} by the fact that the string implementation we used was based on reference counting.
    (If it'd also provide SSO, this would be easy to explain, but the darn thing doesn't.)


    \section{Conclusions}

    The skip list uses the least memory but is awful in every other imaginable way.  The hashmap is
    usually the best, except when we're dealing with a small number of integer keys and doing far more
    inserts than queries.  Note that this doesn't actually happen in practice.

    \bibliographystyle{alpha}

    \bibliography{paper}

\end{document}
