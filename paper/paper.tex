\documentclass[12pt,a4paper]{article}
% vim: set textwidth=100:

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{hyperref}

\title{Associative Data Structures for Website Filtering}
\author{Jelco Bodewes, Pieter Brederode \\ Anton Golov, Timo Koppenberg }

\begin{document}
    \maketitle

    \begin{abstract}
        We have researched the performance of the hash table, binary search tree, and skip list
        associative datastructures in the context of a web filter.  Our measurements were made with
        structures of various sizes (ranging from $2^8$ up to $2^{20}$) and with different possible
        key types (from 4 to 120 bytes).  In this paper, we describe our implementation and
        empirically show that given a sufficiently high number of entries, a hash table has the
        highest insertion and query speeds, while the skip list requires the least memory per
        element.
        % And when I say "hash table", I mean libstdc++'s unordered_map, and when I say bst I mean
        % libstdc++'s map, and if you took different implementations you'd probably get different
        % results...
    \end{abstract}


    \section{Introduction}
    A website filter is a program that categorises websites in order to restrict access to
    designated sites a user might visit. Commonly, the filter operates by keeping a blacklist of
    sites a user may not visit unless they have certain credentials, or alternatively, keeping a
    whitelist and prohibiting any sites not on it.  As the world-wide web is growing, these lists
    have grown larger to a point where the speed of the underlying data structure has become a key
    issue.  The time of both inserting and looking up websites must be kept minimal. Moreover, the
    increased number of entries make memory usage an equally significant concern.

    In technical terms, we are interested in the best data structure for associating many URLs with
    their respective permissions, be it whitelisted or blacklisted for certain users.  We have
    chosen to compare the performance of the binary search tree, the hash map, and the skip list, as
    these are commonly employed in practice.  It is of particular interest how each data structure
    will scale as the number of elements rises.  We have thus chosen for our main research question
    the following:
    % Guys, did you notice we did absolutely nothing about predicting this?

    \begin{quotation}
        How does the number of entries influence the performance of various associative data
        structures?
    \end{quotation}

    While the decision of data structure might mostly be dependant on the number of entries, the
    complexity of the key might also be of concern. Where permissions per page would be preferred, a
    more resource optimal solution might be found in filtering only on domain or even IP adress.  In
    researching the influence of this we formulate a secondary research question:

    \begin{quotation}
        How does the complexity of the key influence the preformance of various associative data structures?
    \end{quotation}

    In this paper, we attempt to answer these questions. Section~\ref{sec:plan} provides details
    about our research plan and the scopes of our research. Section~\ref{sec:implementation} is
    about the implementation of our setup in C++.  The rest of the paper states our results and the
    conclusions that can be drawn from them.

    \section{Research Plan}
    \label{sec:plan}

    \subsubsection*{Scope and assumptions}

    We make several assumptions for our research.

    Firstly we plan on only working with datasets that are certain to fit into the main memory of our machine. We intend to
    purely examine the relative speed and memory-efficiency of the different data structures. Using datasets larger than
    memory will cause regular lookups to the harddrive, which we assume would skew our results.

    In order to make the measurements more predictable, we will always ensure that insertions are only performed with
    non-existent keys. The alternative would make either the memory usage or number of insertions non-deterministic, which
    could cause inaccuracies in measurement. Similarly, we require that all queries be performed on existing keys.

    To mimic real usage, we will insert and then access the keys in an arbitrary order.  However, we will keep the order consistent
    between tests on different data structures.  Together this will minimize any bias related to any kind of order-dependent
    performance, which would likely be an advantage specific to the implementation, not to the data structure. We will
    generate the order for both lookup and insertion by shuffling the dataset we chose for the specific test. This means that
    orders will not be persistent between different data set sizes; however, we do not expect this to be significant.

    We realize that methods to measure the amount of time required for actions will themselves need time. We will however
    use the same time-measuring method for all data structures, so we can still use the results to compare the different
    data structures.

    \section{Implementation}
    \label{sec:implementation}

    We used C++ implementations of the data structures involved, and also the language for our
    measuring code.  We chose C++ as it is an industry standard\footnote{ref}, provides the
    necessary tools, and does not carry the risk of garbage collection pauses or effects due to
    ``warming-up time''.

    \subsection{Data Structures}

    Using C++ as the implementation language, we chose libstdc++'s implementations of the standard class
    templates \texttt{std::map} and \texttt{std::unordered\_map} for the binary search
    tree and the hash table respectively.  As the C++ standard library does not contain a skip
    list\footnote{reference to the standard here}, we used an open-source third party library,
    \texttt{CSSkipList}\footnote{reference to library home page here}.  We also used libstdc++'s
    implementation of \texttt{std::string} for our string handling\footnote{specify?}needs.

    \subsection{Data Sources}
    For our data, we used datasets containing $2^x$ elements with $x$ an integer such that $8 < x <
    21$. Thus we have twelve different dataset sizes. We took the system's dictionary file and the
    list of top level domains from Wikipedia and combined these to generate a list of items in the
    form \texttt{www.\$word.\$tld}, where \texttt{\$word} is one of words from the the dictionary
    and \texttt{\$tld} is a top level domain.  Only words consisting of the lowercase letters
    \texttt{a} through \texttt{z} were used. Constraining the words to consist of 8 to 12 characters
    and limiting the top-level domains to exactly two letters, all possible combinations were
    generated.  For the full paths, a number of domains were selected and each was appended a random
    alphanumeric string of eighty characters.  Finally, for the third type of key, an IP address, we
    used 32 bit integers. The exact implementation can be viewed in the sourcecode\footnote{ref to
    apendix or code dump}.

    \subsection{Measurements}
    \label{subsec:measurements}
    For our time measurements, we used libstdc++'s implementation of the standard
    \texttt{std::chrono::high\_resolution\_clock} class.  There is no official documentation about
    the resolution of this clock, but measurements show that it is likely below one microsecond,
    which is sufficient for our purposes.\footnote{\url{http://stackoverflow.com/a/5524138/559931}}

    Memory measurements were performed by providing a custom allocator which tracked the number of
    memory requests.  Note that the latter deviates from our initial plan, where we
    stated\footnote{ref?}we would use the POSIX API\footnote{ref to the posix api}.  Unfortunately,
    that turned out to be too coarse-grained for our purposes and we could thus get no accurate
    measurements.  The allocator-based technique we employed did require some slight modifications
    to the skip list implementation we used, but this should not have caused a significant slowdown.
    We assume that any influence will only have made the tests more fair, as now allocation was done
    the same way in all three setups.

    After some tests we chose to decrease the number of runs used for measurements.  In the proposal
    we stated that for each time measurement we would perform the relevant operation 1000 times and
    then take the average of that.  In practice, this made the tests run for longer than we could
    afford. Because the results given by performing 100 runs were comparable, we settled for that.
    We expect the general result to be similar; refer to section\footnote{ref}for details about what
    \emph{could} have gone wrong.

    \section{Results}

    Our results were great!  They are best analysed with a two-sample independent T test.  We had an
    equal number of measurements about group 1 (with mean $x_1$ and standard deviation $\sigma_1$ and
    about group 2 (with mean $x_2$ and standard deviation $\sigma_2$).  From this we can conclude that
    data structure 1 is, on average, unquestionably superior.

    \subsection{Errors possibly caused by an imperfect implementation}

    There were a number of measurements that did not follow the underlying theory and are likely a
    consequence of the hardware involved.

    In particular, constructing a small hash table\footnote{state the number of elements in a
    "small" hash table} after constructing many big ones takes a disproportionally long time.  Only
    the first hash table is affected, and the time is comparable to the time required to construct a
    hash table with 4000 elements, which is illogical.  We assume this is caused by the allocator
    but the reason for such behaviour is still unclear.  The fact that a hashtable needs contiguous
    memory is likely to have some influence, as well as the allocator returning memory to the
    operating system.  However, we reason the latter should also have happened with the other
    structures.

    A similar issue occurs around 4000 or 8000 elements (depending on data structure).  For all
    three datastructures, the cost of inserting and querying grows at a higher rate than before.
    This is difficult to quantify\footnote{Can we do some sort of linear regression to prove this is
    the case?}, but it appears some extra load is added at that point.  We suspect it is related
    either to the allocator having to acquire more memory or the CPU cache running out and more
    cache misses occuring.  We would measure this as well, if there was more time.

    A potentially related issue is that the standard deviation of our measurements rises around the
    same point.  That increases the likelihood of it being allocator-related, as that involves a
    system call and could cause scheduling issues\footnote{elaborate/ref?}, despite the system being
    otherwise very close to idle.

    Also, we acquired counterintuitive results showing that inserting or querying a full path is
    faster than querying only the domain name.  This can partially be explained\footnote{Not really,
    but ehhh.}by the fact that the string implementation we used was based on reference counting.
    %(If it'd also provide SSO, this would be easy to explain, but the darn thing doesn't.)

    \section{Conclusions}
    Our main point of research was researching which associative data structure performed best on a
    various number of insertions, in terms of the speed of insertions and queries, and in terms of
    memory necessary per element.  In terms of speed we can state with certainty that the hash table
    is the superior data structure.  It outperforms the other two in almost every category,
    particularly in cases where the number of elements is high, where it is roughly two times faster
    than the skip list and one and a half times faster than the binary search tree.  The binary
    search tree outperforms the hash table when the number of elements is low, and the number of
    insertions is relatively larger than the number of queries.  Whether this situation can occur in
    practice depends on the speed of deletions, which we have not measured; in the given context,
    however, the conditions are unlikely to be achieved.  A skip list may be preferable if memory
    usage is more important than query speeds; given that both insert and query operations take less
    than ten microseconds even in the largest maps we measured, this is likely the case if queries
    and inserts are rare.

    Our secondary point of research encompassed the effect of key complexity on associative data
    structures.  Our different types of keys were IP adresses, domain names and full path URLs.  Our
    measurements confirmed our intuition that IP addresses impose significantly lower costs over the
    other two key types, as well as requiring less memory.  This improvement was more noticable in
    the binary search tree than in the two other systems.  The corresponding expected improvement
    when comparing the domain key type to the full path key type were not observed.

    \section{Reflection}
    In general, we are content with the results of our research. We were able to provide clear and
    concise answers to our research questions. Even though our research questions may appear broad,
    we have defined a good scope in our research proposal and have not altered the overall goal
    of our research.  Errors in the implementation lead us to having to discard measurements of 256
    elements as the outliers were too great to compensate for.  Fortunately, our focus lay primarily
    on the larger datasets and thus this is not significant for our conclusions.  The implementation
    we finally arrived at differs from what we outlined, but the changes are largely to increase the
    accuracy of measurements or the speed at which measurements can be obtained, and have no
    fundamental influence on the results.  The primary difficulty we encountered was the number of
    dimensions in our data: with 117 independent measurements, it took considerable time to perform
    them all, and then to perform statistical analysis on them.

    Knowing the strengths and weaknesses of the various data structures, further research could
    compare subvariants of the hash table and skip list.  In particular, it may be of interest to
    experiment with differen choices of hash function and collision resolution scheme.  Larger
    numbers of elements may also provide interesting challenges, as the number of elements may be
    too great to keep in main memory.  Finally, the situation where a map is accessed by multiple
    threads concurrently provides extra complications that could be considered.

    \bibliographystyle{alpha}

    \bibliography{paper}

\end{document}
